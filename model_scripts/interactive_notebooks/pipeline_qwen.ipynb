{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies \n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_dir = snapshot_download('qwen/Qwen-VL')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"cuda:0\",\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "tokenizer.pad_token = '<|endoftext|>'\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_type = \"complex\"\n",
    "question_type = \"complex\"\n",
    "\n",
    "os.makedirs(f\"../Results/Qwen-VL/{chart_type}_{question_type}/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import re\n",
    "  \n",
    "def modified_relaxed_accuracy(question:str,\n",
    "                        target: str,\n",
    "                        prediction: str,\n",
    "                        max_relative_change: float = 0.05) -> bool:\n",
    "  \"\"\"Calculates relaxed correctness.\n",
    "\n",
    "  The correctness tolerates certain error ratio defined by max_relative_change.\n",
    "  See https://arxiv.org/pdf/2203.10244.pdf, end of section 5.1:\n",
    "  “Following Methani et al. (2020), we use a relaxed accuracy measure for the\n",
    "  numeric answers to allow a minor inaccuracy that may result from the automatic\n",
    "  data extraction process. We consider an answer to be correct if it is within\n",
    "  5% of the gold answer. For non-numeric answers, we still need an exact match\n",
    "  to consider an answer to be correct. \n",
    "  This is now updated to take in account a lot more cases”\n",
    "\n",
    "  Args:\n",
    "    target: Target string.\n",
    "    prediction: Predicted string.\n",
    "    max_relative_change: Maximum relative change.\n",
    "\n",
    "  Returns:\n",
    "    Whether the prediction was correct given the specified tolerance.\n",
    "  \"\"\"\n",
    "  def _to_float(text: str) -> Optional[float]:\n",
    "    try:\n",
    "      if text.endswith(\"%\"):\n",
    "        return float(text.rstrip(\"%\")) / 100.0\n",
    "      else:\n",
    "        return float(text)\n",
    "    except ValueError:\n",
    "      return None\n",
    "    \n",
    "  def _remove_commas_from_numbers(text: str) -> str:\n",
    "    text = re.sub(r'(\\d*),(\\d+)', r'\\1\\2', text)\n",
    "    return text\n",
    "  \n",
    "  def _remove_spaces(text: str) -> str:\n",
    "    return text.replace(\" \", \"\")\n",
    "  \n",
    "  def _check_for_years(question: str) -> bool:\n",
    "    return \"year\" in question.lower()\n",
    "  \n",
    "  def _check_list(text: str) -> bool:\n",
    "    return text.startswith(\"[\") and text.endswith(\"]\")\n",
    "  \n",
    "  def _list_of_answers(target: str, prediction: str) -> bool:\n",
    "    target = target.split(\",\")\n",
    "    prediction = prediction.split(\",\")\n",
    "    target = sorted(target)\n",
    "    prediction = sorted(prediction)\n",
    "    return target == prediction\n",
    "  \n",
    "  prediction = _remove_commas_from_numbers(prediction)\n",
    "  target = _remove_commas_from_numbers(target)\n",
    "  \n",
    "  prediction_float = _to_float(prediction)\n",
    "  target_float = _to_float(target)\n",
    "  \n",
    "  if not _check_for_years(question) and (prediction_float is not None and target_float is not None):\n",
    "    try :\n",
    "      relative_change = abs(prediction_float - target_float) / abs(target_float)\n",
    "      return relative_change <= max_relative_change\n",
    "    except :\n",
    "      return False\n",
    "  else:\n",
    "    prediction = _remove_spaces(prediction)\n",
    "    target = _remove_spaces(target)\n",
    "    \n",
    "    if _check_list(target) and _check_list(prediction):\n",
    "      return _list_of_answers(target[1:-1], prediction[1:-1])\n",
    "    elif _check_list(target) or _check_list(prediction):\n",
    "        return _list_of_answers(target[1:-1], prediction) or _list_of_answers(target, prediction[1:-1])\n",
    "    else:\n",
    "      return target.lower() in prediction.lower() or (prediction.lower() in target.lower() and len(prediction) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = os.listdir(\"../perturb_jsons/{}_{}\".format(chart_type, question_type))\n",
    "categories = [os.path.basename(category).split(\".\")[0] for category in categories]\n",
    "categories = sorted(categories)\n",
    "\n",
    "global_answers = {}\n",
    "category_wise_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    print(\"running for category:\", category)\n",
    "    print()\n",
    "    df = pd.read_json('../perturb_jsons/{}_{}/{}.json'.format(chart_type, question_type, category))\n",
    "    questions = df['query'].tolist()\n",
    "    gold_labels = df['label'].tolist()\n",
    "    imagenames = df['imgname'].tolist()\n",
    "    perturbations = df['perturbation'].tolist()\n",
    "    imagenames = [f'../final_data/{chart_type}_{question_type}/plots/{pert}/{img}.png' for img, pert in zip(imagenames, perturbations)]\n",
    "    \n",
    "    queries = []\n",
    "    for i, question in enumerate(questions):\n",
    "        text = question + ' Answer:'\n",
    "        queries.append(tokenizer.from_list_format([\n",
    "                {'image': imagenames[i]},\n",
    "                {'text': text},\n",
    "        ]))  \n",
    "\n",
    "    batch_size = 8\n",
    "    batches = [queries[i:i+batch_size] for i in range(0, len(queries), batch_size)] \n",
    "       \n",
    "    model_responses = []\n",
    "    for batch in batches:\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True)\n",
    "        inputs.to(model.device)\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                outputs = model.generate(**inputs)\n",
    "            except:\n",
    "                print(\"bad output\")\n",
    "        generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        model_responses.extend(generated)\n",
    "        print(\".\" * batch_size, end='')\n",
    "    print()\n",
    "        \n",
    "    global_answers[category] = model_responses\n",
    "    print(\"answers generated!\")\n",
    "\n",
    "    results = list(zip(questions, model_responses))\n",
    "    final_responses = []\n",
    "    for result in results:\n",
    "        question, response = result\n",
    "        new_response = response.split('Answer:')[-1].strip()\n",
    "        new_response = new_response.split('%')[0].strip()\n",
    "        final_responses.append(new_response)\n",
    "        \n",
    "    final_responses = [response.split('=')[-1] for response in final_responses]\n",
    "    final_responses = [response.split('%')[0] for response in final_responses]\n",
    "\n",
    "    model_performance = []\n",
    "    results = list(zip(questions, model_responses))\n",
    "    for i, ans in enumerate(final_responses):\n",
    "        model_score = modified_relaxed_accuracy(questions[i],gold_labels[i], ans)\n",
    "        model_performance.append(model_score)\n",
    "\n",
    "    category_wise_scores[category] = sum(model_performance)\n",
    "    print('Model accuracy:', sum(model_performance) / len(model_performance))\n",
    "    print()\n",
    "    \n",
    "    # store resutls after each category\n",
    "    with open(f\"../Results/Qwen-VL/{chart_type}_{question_type}/results.json\", \"w\") as f:\n",
    "        json.dump(category_wise_scores, f)\n",
    "    # store answers after each category\n",
    "    with open(f\"../Results/Qwen-VL/{chart_type}_{question_type}/answers.json\", \"w\") as f:\n",
    "        json.dump(global_answers, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chart-rob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
