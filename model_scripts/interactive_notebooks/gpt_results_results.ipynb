{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_type = \"complex\"\n",
    "ques_type = \"complex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import re\n",
    "  \n",
    "def modified_relaxed_accuracy(question:str,\n",
    "                        target: str,\n",
    "                        prediction: str,\n",
    "                        max_relative_change: float = 0.05) -> bool:\n",
    "  \"\"\"Calculates relaxed correctness.\n",
    "\n",
    "  The correctness tolerates certain error ratio defined by max_relative_change.\n",
    "  See https://arxiv.org/pdf/2203.10244.pdf, end of section 5.1:\n",
    "  “Following Methani et al. (2020), we use a relaxed accuracy measure for the\n",
    "  numeric answers to allow a minor inaccuracy that may result from the automatic\n",
    "  data extraction process. We consider an answer to be correct if it is within\n",
    "  5% of the gold answer. For non-numeric answers, we still need an exact match\n",
    "  to consider an answer to be correct. \n",
    "  This is now updated to take in account a lot more cases”\n",
    "\n",
    "  Args:\n",
    "    target: Target string.\n",
    "    prediction: Predicted string.\n",
    "    max_relative_change: Maximum relative change.\n",
    "\n",
    "  Returns:\n",
    "    Whether the prediction was correct given the specified tolerance.\n",
    "  \"\"\"\n",
    "  def _to_float(text: str) -> Optional[float]:\n",
    "    try:\n",
    "      if text.endswith(\"%\"):\n",
    "        return float(text.rstrip(\"%\")) / 100.0\n",
    "      else:\n",
    "        return float(text)\n",
    "    except ValueError:\n",
    "      return None\n",
    "    \n",
    "  def _remove_commas_from_numbers(text: str) -> str:\n",
    "    text = re.sub(r'(\\d*),(\\d+)', r'\\1\\2', text)\n",
    "    return text\n",
    "  \n",
    "  def _remove_spaces(text: str) -> str:\n",
    "    return text.replace(\" \", \"\")\n",
    "  \n",
    "  def _check_for_years(question: str) -> bool:\n",
    "    return \"year\" in question.lower()\n",
    "  \n",
    "  def _check_list(text: str) -> bool:\n",
    "    return text.startswith(\"[\") and text.endswith(\"]\")\n",
    "  \n",
    "  def _list_of_answers(target: str, prediction: str) -> bool:\n",
    "    target = target.split(\",\")\n",
    "    prediction = prediction.split(\",\")\n",
    "    target = sorted(target)\n",
    "    prediction = sorted(prediction)\n",
    "    return target == prediction\n",
    "  \n",
    "  prediction = _remove_commas_from_numbers(prediction)\n",
    "  target = _remove_commas_from_numbers(target)\n",
    "  \n",
    "  prediction_float = _to_float(prediction)\n",
    "  target_float = _to_float(target)\n",
    "  \n",
    "  if not _check_for_years(question) and (prediction_float is not None and target_float is not None):\n",
    "    try :\n",
    "      relative_change = abs(prediction_float - target_float) / abs(target_float)\n",
    "      return relative_change <= max_relative_change\n",
    "    except :\n",
    "      return False\n",
    "  else:\n",
    "    prediction = _remove_spaces(prediction)\n",
    "    target = _remove_spaces(target)\n",
    "    \n",
    "    if _check_list(target) and _check_list(prediction):\n",
    "      return _list_of_answers(target[1:-1], prediction[1:-1])\n",
    "    elif _check_list(target) or _check_list(prediction):\n",
    "        return _list_of_answers(target[1:-1], prediction) or _list_of_answers(target, prediction[1:-1])\n",
    "    else:\n",
    "      return target.lower() in prediction.lower() or (prediction.lower() in target.lower() and len(prediction) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = os.listdir(\"../perturb_jsons/{}_{}\".format(chart_type, ques_type))\n",
    "categories = [x.split('.')[0] for x in categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    all_responses = pd.read_json(f\"./GPT_final_output/{chart_type}_{ques_type}/{category}.jsonl\", lines=True)\n",
    "    responses = all_responses['response']\n",
    "\n",
    "    df = pd.read_json(f\"../perturb_jsons/{chart_type}_{ques_type}/{category}.json\")\n",
    "    questions = df['query'].tolist()\n",
    "    gold_labels = df['label'].tolist()\n",
    "\n",
    "    model_responses = [one_resp['body']['choices'][0]['message']['content'] for one_resp in responses] \n",
    "\n",
    "    copy = model_responses.copy()\n",
    "    for i, resp in enumerate(copy):\n",
    "        if(resp[-1] == '.'):\n",
    "            resp = resp[:-1]\n",
    "        if 'The answer is: ' in resp:\n",
    "            x = resp.split('The answer is: ')\n",
    "            model_responses[i] = x[1]\n",
    "        elif 'the answer is: ' in resp:\n",
    "            x = resp.split('the answer is: ')\n",
    "            model_responses[i] = x[1]\n",
    "        elif 'The answer is ' in resp:\n",
    "            x = resp.split('The answer is ')\n",
    "            model_responses[i] = x[1]\n",
    "        else:\n",
    "            print(\"!!!\")\n",
    "\n",
    "    results = list(zip(questions, model_responses))\n",
    "    final_responses = []\n",
    "    for result in results:\n",
    "        question, response = result\n",
    "        final_responses.append(response.strip())\n",
    "        \n",
    "    final_responses = [response.split('=')[-1] for response in final_responses]\n",
    "    final_responses = [response.split('%')[0] for response in final_responses]\n",
    "\n",
    "    model_performance = []\n",
    "    results = list(zip(questions, model_responses))\n",
    "    for i, ans in enumerate(final_responses):\n",
    "        model_score = modified_relaxed_accuracy(questions[i],gold_labels[i], ans)\n",
    "        model_performance.append(model_score)\n",
    "\n",
    "    print('Category:', category)\n",
    "    print('Model Performance:', sum(model_performance))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
